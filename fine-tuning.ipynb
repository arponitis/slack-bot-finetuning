{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4gtGc618Bd4W/x3Sk1Ygi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a58c7f4"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "761fe09d"
      },
      "source": [
        "!pip install -q transformers peft datasets bitsandbytes accelerate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fbc51d3"
      },
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7686f356"
      },
      "source": [
        "import json\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0d13bf5"
      },
      "source": [
        "## 3. Load & Preprocess Chat Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f171296e"
      },
      "source": [
        "# Example: Slack JSON export (replace 'slack_export.json' with your file)\n",
        "# Slack/Teams exports usually look like:\n",
        "# [\n",
        "#   {\"user\": \"Alice\", \"text\": \"Hello Bob\", \"ts\": \"1680000000.000000\"},\n",
        "#   {\"user\": \"Bob\", \"text\": \"Hey Alice!\", \"ts\": \"1680000001.000000\"}\n",
        "# ]\n",
        "#\n",
        "# Upload your export file to Colab: Runtime → Files → Upload\n",
        "CHAT_FILE = \"/content/slack_export.json\"  # change this\n",
        "\n",
        "with open(CHAT_FILE, \"r\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Remove system messages and empty texts\n",
        "cleaned = [\n",
        "    msg for msg in raw_data\n",
        "    if msg.get(\"text\") and not msg[\"text\"].startswith(\"<@\") and \"joined\" not in msg[\"text\"].lower()\n",
        "]\n",
        "\n",
        "# Convert into conversation turns (instruction → response)\n",
        "pairs = []\n",
        "for i in range(len(cleaned) - 1):\n",
        "    current = cleaned[i]\n",
        "    nxt = cleaned[i + 1]\n",
        "    if current[\"user\"] != nxt[\"user\"]:  # only consecutive different speakers\n",
        "        instruction = f\"{current['user']}: {current['text']}\\n{nxt['user']}:\"\n",
        "        output = nxt[\"text\"]\n",
        "        pairs.append({\n",
        "            \"instruction\": \"Respond in the style of our team chat\",\n",
        "            \"input\": instruction,\n",
        "            \"output\": output\n",
        "        })\n",
        "\n",
        "print(f\"Prepared {len(pairs)} conversation pairs.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6366caa9"
      },
      "source": [
        "## 4. Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3134ea2f"
      },
      "source": [
        "dataset = Dataset.from_list(pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68a0d2b7"
      },
      "source": [
        "## 5. Tokenization Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57d98c8c"
      },
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_fn(example):\n",
        "    prompt = f\"{example['instruction']}\\n{example['input']}\"\n",
        "    labels = example['output']\n",
        "    text = f\"{prompt}\\n{labels}\"\n",
        "    tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=512)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cf3e0c9"
      },
      "source": [
        "## 6. Load Model with QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bdd8198"
      },
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    #load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f79316c"
      },
      "source": [
        "## 7. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6330c3a"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"tinyllama-slack\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "391f321a"
      },
      "source": [
        "## 8. Save LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bb46fc2"
      },
      "source": [
        "model.save_pretrained(\"tinyllama-slack-lora\")\n",
        "tokenizer.save_pretrained(\"tinyllama-slack-lora\")\n",
        "print(\"✅ Fine-tuning complete. Adapters saved to 'tinyllama-slack-lora'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fa6cbd2"
      },
      "source": [
        "## 9. Inference Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf503b23"
      },
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"tinyllama-slack-lora\")\n",
        "\n",
        "prompt = \"You are answering a question like a conversation.\\n Dana: We need to update the docs. \\nAlice:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(ft_model.device)\n",
        "outputs = ft_model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}